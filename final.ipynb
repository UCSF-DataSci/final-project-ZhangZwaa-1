{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f08b31d",
   "metadata": {},
   "source": [
    "## Download dataset and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b136e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PASCAL VOC 2012 training dataset (filtering for ['person'], resizing to (448, 448), and applying CLAHE)...\n",
      "Training dataset loaded: 5717 samples\n",
      "\n",
      "Loading PASCAL VOC 2012 validation dataset (filtering for ['person'], resizing to (448, 448), and applying CLAHE)...\n",
      "Validation dataset loaded: 5823 samples\n",
      "\n",
      "Displaying example images with CLAHE applied and filtered annotations...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CLAHE applied. Images are now uniformly resized, and bounding boxes are scaled accordingly.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import VOCDetection\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image # Pillow for PIL Image handling\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "import cv2 # Import OpenCV\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_DIR = './data_voc'\n",
    "YEAR = '2012'\n",
    "DOWNLOAD = True\n",
    "\n",
    "TARGET_CLASSES = ['person'] \n",
    "IMAGE_SIZE = (448, 448) \n",
    "\n",
    "# --- Custom Transform to Filter Annotations and Resize Images with Bounding Boxes ---\n",
    "class FilterAndResizeVOCClasses:\n",
    "    def __init__(self, target_classes, image_size, apply_clahe=True): # Added apply_clahe parameter\n",
    "        self.target_classes = target_classes\n",
    "        self.image_size = image_size\n",
    "        self.apply_clahe = apply_clahe\n",
    "\n",
    "        # Define CLAHE object (only if apply_clahe is True)\n",
    "        if self.apply_clahe:\n",
    "            # Create a CLAHE object (clipLimit: contrast limiting threshold, tileGridSize: size of grid for histogram equalization)\n",
    "            self.clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8)) # Common parameters\n",
    "\n",
    "        self.image_transform_pre_clahe = transforms.Compose([\n",
    "            transforms.Resize(image_size) # Resize first\n",
    "        ])\n",
    "        self.image_transform_post_clahe = transforms.Compose([\n",
    "            transforms.ToTensor() # Convert to Tensor after CLAHE (if applied)\n",
    "        ])\n",
    "\n",
    "    def __call__(self, img, target):\n",
    "        original_width = int(target['annotation']['size']['width'])\n",
    "        original_height = int(target['annotation']['size']['height'])\n",
    "\n",
    "        # 1. Apply initial image transform (Resize)\n",
    "        img = self.image_transform_pre_clahe(img) # img is still a PIL Image here\n",
    "\n",
    "        # 2. Apply CLAHE (if enabled)\n",
    "        if self.apply_clahe:\n",
    "            # Convert PIL Image to OpenCV format (numpy array)\n",
    "            img_np = np.array(img)\n",
    "            # CLAHE usually works on grayscale or L*a*b* L-channel\n",
    "            # Convert to grayscale for CLAHE\n",
    "            if len(img_np.shape) == 3 and img_np.shape[2] == 3: # Check if it's an RGB image\n",
    "                img_gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)\n",
    "            else: # Already grayscale or single channel\n",
    "                img_gray = img_np\n",
    "\n",
    "            # Apply CLAHE\n",
    "            clahe_img = self.clahe.apply(img_gray)\n",
    "            \n",
    "            # If original was RGB, convert back to RGB (by stacking grayscale or converting from L channel)\n",
    "            if len(img_np.shape) == 3 and img_np.shape[2] == 3:\n",
    "                 # Stack the CLAHE-enhanced grayscale image to form an RGB image for consistent input to ToTensor\n",
    "                clahe_img_rgb = cv2.cvtColor(clahe_img, cv2.COLOR_GRAY2RGB)\n",
    "            else:\n",
    "                clahe_img_rgb = clahe_img # Already single channel or expected to be\n",
    "\n",
    "            # Convert back to PIL Image for further torchvision transforms\n",
    "            img = Image.fromarray(clahe_img_rgb)\n",
    "\n",
    "        # 3. Apply final image transform (ToTensor)\n",
    "        img = self.image_transform_post_clahe(img)\n",
    "\n",
    "        # --- Bounding box filtering and scaling (same as before) ---\n",
    "        filtered_target = copy.deepcopy(target)\n",
    "        objects = filtered_target['annotation']['object']\n",
    "        if not isinstance(objects, list):\n",
    "            objects = [objects]\n",
    "        filtered_objects = []\n",
    "        for obj in objects:\n",
    "            if obj['name'] in self.target_classes:\n",
    "                bndbox = obj['bndbox']\n",
    "                xmin = float(bndbox['xmin'])\n",
    "                ymin = float(bndbox['ymin'])\n",
    "                xmax = float(bndbox['xmax'])\n",
    "                ymax = float(bndbox['ymax'])\n",
    "                scale_x = self.image_size[1] / original_width\n",
    "                scale_y = self.image_size[0] / original_height\n",
    "                obj['bndbox']['xmin'] = int(xmin * scale_x)\n",
    "                obj['bndbox']['ymin'] = int(ymin * scale_y)\n",
    "                obj['bndbox']['xmax'] = int(xmax * scale_x)\n",
    "                obj['bndbox']['ymax'] = int(ymax * scale_y)\n",
    "                obj['bndbox']['xmin'] = max(0, obj['bndbox']['xmin'])\n",
    "                obj['bndbox']['ymin'] = max(0, obj['bndbox']['ymin'])\n",
    "                obj['bndbox']['xmax'] = min(self.image_size[1] - 1, obj['bndbox']['xmax'])\n",
    "                obj['bndbox']['ymax'] = min(self.image_size[0] - 1, obj['bndbox']['ymax'])\n",
    "                obj['bndbox']['xmax'] = max(obj['bndbox']['xmin'], obj['bndbox']['xmax'])\n",
    "                obj['bndbox']['ymax'] = max(obj['bndbox']['ymin'], obj['bndbox']['ymax'])\n",
    "                filtered_objects.append(obj)\n",
    "        filtered_target['annotation']['object'] = filtered_objects\n",
    "\n",
    "        return img, filtered_target\n",
    "\n",
    "# Instantiate our custom transform with CLAHE enabled\n",
    "filter_and_resize_transform = FilterAndResizeVOCClasses(TARGET_CLASSES, IMAGE_SIZE, apply_clahe=True)\n",
    "\n",
    "# --- Define a custom collate_fn for DataLoader (same as before) ---\n",
    "def custom_collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch], 0)\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets\n",
    "\n",
    "\n",
    "# --- 1. Import and Dataset Splitting (same as before) ---\n",
    "print(f\"Loading PASCAL VOC {YEAR} training dataset (filtering for {TARGET_CLASSES}, resizing to {IMAGE_SIZE}, and applying CLAHE)...\")\n",
    "train_dataset = VOCDetection(\n",
    "    root=DATA_DIR,\n",
    "    year=YEAR,\n",
    "    image_set='train',\n",
    "    download=DOWNLOAD,\n",
    "    transforms=filter_and_resize_transform\n",
    ")\n",
    "print(f\"Training dataset loaded: {len(train_dataset)} samples\")\n",
    "\n",
    "print(f\"\\nLoading PASCAL VOC {YEAR} validation dataset (filtering for {TARGET_CLASSES}, resizing to {IMAGE_SIZE}, and applying CLAHE)...\")\n",
    "val_dataset = VOCDetection(\n",
    "    root=DATA_DIR,\n",
    "    year=YEAR,\n",
    "    image_set='val',\n",
    "    download=DOWNLOAD,\n",
    "    transforms=filter_and_resize_transform\n",
    ")\n",
    "print(f\"Validation dataset loaded: {len(val_dataset)} samples\")\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0, collate_fn=custom_collate_fn)\n",
    "\n",
    "\n",
    "# --- 2. Display Example Images (same as before) ---\n",
    "print(\"\\nDisplaying example images with CLAHE applied and filtered annotations...\")\n",
    "\n",
    "def to_numpy_image(tensor_image):\n",
    "    return tensor_image.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "dataiter = iter(val_loader)\n",
    "images, targets = next(dataiter)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    ax = plt.subplot(1, len(images), i + 1)\n",
    "    img_tensor = images[i]\n",
    "    img_np = to_numpy_image(img_tensor)\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "    img_target = targets[i]\n",
    "    objects = img_target['annotation']['object']\n",
    "    if not isinstance(objects, list):\n",
    "        objects = [objects]\n",
    "\n",
    "    if not objects:\n",
    "        plt.title(f\"Image {i+1}\\n(No {TARGET_CLASSES} objects found)\")\n",
    "        continue\n",
    "\n",
    "    for obj in objects:\n",
    "        label = obj['name']\n",
    "        bndbox = obj['bndbox']\n",
    "        xmin = int(bndbox['xmin'])\n",
    "        ymin = int(bndbox['ymin'])\n",
    "        xmax = int(bndbox['xmax'])\n",
    "        ymax = int(bndbox['ymax'])\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin),\n",
    "            xmax - xmin,\n",
    "            ymax - ymin,\n",
    "            linewidth=2,\n",
    "            edgecolor='g',\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        plt.text(\n",
    "            xmin, ymin - 5,\n",
    "            label,\n",
    "            color='g',\n",
    "            fontsize=10,\n",
    "            weight='bold',\n",
    "            bbox=dict(facecolor='lightgreen', alpha=0.5, edgecolor='none', pad=0)\n",
    "        )\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCLAHE applied. Images are now uniformly resized, and bounding boxes are scaled accordingly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0442882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Part 2: Displaying Example Images with Filtered and Resized Annotations ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Part 2: Displaying Example Images\n",
    "\n",
    "# --- 2. Display Example Images ---\n",
    "print(\"\\n--- Part 2: Displaying Example Images with Filtered and Resized Annotations ---\")\n",
    "\n",
    "# Helper function to convert tensor image back to numpy for plotting\n",
    "def to_numpy_image(tensor_image):\n",
    "    # Permute dimensions from (C, H, W) to (H, W, C) for matplotlib\n",
    "    # Clamp values to [0, 1] in case of any floating point issues after transformations\n",
    "    return tensor_image.permute(1, 2, 0).numpy().clip(0, 1)\n",
    "\n",
    "# Get a batch of images and targets from the validation loader\n",
    "# This line should now work correctly, thanks to the custom collate_fn\n",
    "dataiter = iter(val_loader)\n",
    "images, targets = next(dataiter)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i in range(len(images)):\n",
    "    ax = plt.subplot(1, len(images), i + 1)\n",
    "    img_tensor = images[i]\n",
    "    img_np = to_numpy_image(img_tensor)\n",
    "    plt.imshow(img_np)\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Get annotations for the current image. targets is now a list of dicts.\n",
    "    img_target = targets[i]\n",
    "    \n",
    "    # The 'object' key will now only contain 'person' objects,\n",
    "    # and their bounding box coordinates will be scaled to IMAGE_SIZE.\n",
    "    objects = img_target['annotation']['object']\n",
    "    if not isinstance(objects, list): # Handle case where there's only one object (after filtering)\n",
    "        objects = [objects]\n",
    "\n",
    "    if not objects: # If no 'person' objects are left after filtering for this image\n",
    "        plt.title(f\"Image {i+1}\\n(No {TARGET_CLASSES} objects found)\")\n",
    "        continue\n",
    "\n",
    "    for obj in objects:\n",
    "        label = obj['name']\n",
    "        bndbox = obj['bndbox']\n",
    "        xmin = int(bndbox['xmin'])\n",
    "        ymin = int(bndbox['ymin'])\n",
    "        xmax = int(bndbox['xmax'])\n",
    "        ymax = int(bndbox['ymax'])\n",
    "\n",
    "        # Create a Rectangle patch for the bounding box\n",
    "        rect = patches.Rectangle(\n",
    "            (xmin, ymin),             # (x,y) lower-left corner\n",
    "            xmax - xmin,              # width\n",
    "            ymax - ymin,              # height\n",
    "            linewidth=2,\n",
    "            edgecolor='g',            # Green box for person\n",
    "            facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect) # Add the patch to the plot\n",
    "\n",
    "        # Add label text above the bounding box\n",
    "        plt.text(\n",
    "            xmin, ymin - 5,           # Position for text\n",
    "            label,\n",
    "            color='g',                # Green text\n",
    "            fontsize=10,\n",
    "            weight='bold',\n",
    "            bbox=dict(facecolor='lightgreen', alpha=0.5, edgecolor='none', pad=0) # Light green background for text\n",
    "        )\n",
    "    plt.title(f\"Image {i+1}\")\n",
    "\n",
    "plt.tight_layout() # Adjust subplot params for a tight layout\n",
    "plt.show() # Display the plot\n",
    "\n",
    "# print(\"\\n--- Part 2: Displaying Example Images Complete ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c5e4de",
   "metadata": {},
   "source": [
    "# YOLOV8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e9e1d69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PASCAL VOC to YOLO format conversion...\n",
      "Processing image set: train...\n",
      "Processing image set: val...\n",
      "Conversion complete.\n",
      "\n",
      "Verify your data structure in ./yolov8_pascal_person_data:\n",
      "  ./yolov8_pascal_person_data/images/train/\n",
      "  ./yolov8_pascal_person_data/images/val/\n",
      "  ./yolov8_pascal_person_data/labels/train/\n",
      "  ./yolov8_pascal_person_data/labels/val/\n",
      "  (Each image in 'images' should have a corresponding .txt file in 'labels')\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from shutil import copyfile\n",
    "\n",
    "# --- Configuration for PASCAL VOC and YOLO Conversion ---\n",
    "VOC_ROOT_DIR = './data_voc/VOCdevkit/VOC2012' # Path to your downloaded VOC2012 data\n",
    "OUTPUT_YOLO_DIR = './yolov8_pascal_person_data' # Where your YOLO formatted data will go\n",
    "\n",
    "TARGET_CLASSES = ['person'] # Only convert annotations for this class\n",
    "# Map class names to YOLO-compatible integer IDs.\n",
    "# Since we only have 'person', its ID will be 0.\n",
    "CLASS_NAME_TO_ID = {name: i for i, name in enumerate(TARGET_CLASSES)}\n",
    "# Example if you had more: {'person': 0, 'car': 1, 'aeroplane': 2}\n",
    "\n",
    "def convert_bbox_to_yolo(size, box):\n",
    "    \"\"\"\n",
    "    Converts [xmin, ymin, xmax, ymax] to [x_center, y_center, width, height]\n",
    "    all normalized by image dimensions.\n",
    "    \"\"\"\n",
    "    dw = 1. / size[0] # 1 / width\n",
    "    dh = 1. / size[1] # 1 / height\n",
    "    x = (box[0] + box[1]) / 2.0 # (xmin + xmax) / 2\n",
    "    y = (box[2] + box[3]) / 2.0 # (ymin + ymax) / 2\n",
    "    w = box[1] - box[0]        # xmax - xmin\n",
    "    h = box[3] - box[2]        # ymax - ymin\n",
    "    x = x * dw\n",
    "    w = w * dw\n",
    "    y = y * dh\n",
    "    h = h * dh\n",
    "    return (x, y, w, h)\n",
    "\n",
    "def process_image_set(image_set_name, output_base_dir):\n",
    "    \"\"\"\n",
    "    Processes a given image set (e.g., 'train', 'val') from VOC to YOLO format.\n",
    "    \"\"\"\n",
    "    print(f\"Processing image set: {image_set_name}...\")\n",
    "\n",
    "    # Define paths for VOC\n",
    "    images_dir = os.path.join(VOC_ROOT_DIR, 'JPEGImages')\n",
    "    annotations_dir = os.path.join(VOC_ROOT_DIR, 'Annotations')\n",
    "    image_set_path = os.path.join(VOC_ROOT_DIR, 'ImageSets', 'Main', f'{image_set_name}.txt')\n",
    "\n",
    "    # Define output paths for YOLO format\n",
    "    output_images_dir = os.path.join(output_base_dir, 'images', image_set_name)\n",
    "    output_labels_dir = os.path.join(output_base_dir, 'labels', image_set_name)\n",
    "    \n",
    "    os.makedirs(output_images_dir, exist_ok=True)\n",
    "    os.makedirs(output_labels_dir, exist_ok=True)\n",
    "\n",
    "    with open(image_set_path, 'r') as f:\n",
    "        image_ids = [line.strip() for line in f.readlines()]\n",
    "\n",
    "    for img_id in image_ids:\n",
    "        xml_path = os.path.join(annotations_dir, f'{img_id}.xml')\n",
    "        img_path = os.path.join(images_dir, f'{img_id}.jpg')\n",
    "        label_output_path = os.path.join(output_labels_dir, f'{img_id}.txt')\n",
    "        img_output_path = os.path.join(output_images_dir, f'{img_id}.jpg')\n",
    "\n",
    "        try:\n",
    "            tree = ET.parse(xml_path)\n",
    "            root = tree.getroot()\n",
    "\n",
    "            size = root.find('size')\n",
    "            img_width = int(size.find('width').text)\n",
    "            img_height = int(size.find('height').text)\n",
    "\n",
    "            yolo_annotations = []\n",
    "            for obj in root.findall('object'):\n",
    "                obj_name = obj.find('name').text\n",
    "                if obj_name in TARGET_CLASSES:\n",
    "                    class_id = CLASS_NAME_TO_ID[obj_name]\n",
    "                    \n",
    "                    bndbox = obj.find('bndbox')\n",
    "                    xmin = float(bndbox.find('xmin').text)\n",
    "                    ymin = float(bndbox.find('ymin').text)\n",
    "                    xmax = float(bndbox.find('xmax').text)\n",
    "                    ymax = float(bndbox.find('ymax').text)\n",
    "\n",
    "                    # VOC bbox are 1-indexed, so convert to 0-indexed for width/height calculation if needed\n",
    "                    # (xmin, ymin, xmax, ymax) as floats\n",
    "                    bbox_voc = (xmin, xmax, ymin, ymax) # (xmin, xmax, ymin, ymax) for convert_bbox_to_yolo\n",
    "                    yolo_bbox = convert_bbox_to_yolo((img_width, img_height), bbox_voc)\n",
    "                    \n",
    "                    yolo_annotations.append(f\"{class_id} {yolo_bbox[0]:.6f} {yolo_bbox[1]:.6f} {yolo_bbox[2]:.6f} {yolo_bbox[3]:.6f}\")\n",
    "            \n",
    "            # Only create label file and copy image if there are target objects found\n",
    "            if yolo_annotations:\n",
    "                with open(label_output_path, 'w') as f:\n",
    "                    f.write('\\n'.join(yolo_annotations))\n",
    "                copyfile(img_path, img_output_path)\n",
    "            # else: # Optional: if you want to explicitly exclude images with no target objects\n",
    "            #     print(f\"Skipping {img_id}: no target objects found.\")\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning: XML or image file not found for {img_id}. Skipping.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {img_id}: {e}. Skipping.\")\n",
    "\n",
    "# --- Run the conversion ---\n",
    "print(\"Starting PASCAL VOC to YOLO format conversion...\")\n",
    "# PASCAL VOC has 'train', 'val', and 'trainval' image sets.\n",
    "# We'll convert 'train' and 'val' for YOLO training and validation.\n",
    "process_image_set('train', OUTPUT_YOLO_DIR)\n",
    "process_image_set('val', OUTPUT_YOLO_DIR)\n",
    "print(\"Conversion complete.\")\n",
    "\n",
    "# --- Verify the directory structure ---\n",
    "print(f\"\\nVerify your data structure in {OUTPUT_YOLO_DIR}:\")\n",
    "print(f\"  {OUTPUT_YOLO_DIR}/images/train/\")\n",
    "print(f\"  {OUTPUT_YOLO_DIR}/images/val/\")\n",
    "print(f\"  {OUTPUT_YOLO_DIR}/labels/train/\")\n",
    "print(f\"  {OUTPUT_YOLO_DIR}/labels/val/\")\n",
    "print(f\"  (Each image in 'images' should have a corresponding .txt file in 'labels')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1545b377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLOv8n model...\n",
      "Starting YOLOv8 model training...\n",
      "Ultralytics 8.3.151  Python-3.12.10 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4080 SUPER, 16376MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=-1, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=D:\\homework\\DS223\\final-project-ZhangZwaa-1\\yolov8_pascal_person_data\\a.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=448, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=yolov8n_person_voc, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=d:\\homework\\runs\\detect\\yolov8n_person_voc, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    751507  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,011,043 parameters, 3,011,027 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.1 ms, read: 25.911.7 MB/s, size: 87.0 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\homework\\DS223\\final-project-ZhangZwaa-1\\yolov8_pascal_person_data\\labels\\train.cache... 2142 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2142/2142 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mComputing optimal batch size for imgsz=448 at 60.0% CUDA memory utilization.\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mCUDA:0 (NVIDIA GeForce RTX 4080 SUPER) 15.99G total, 0.46G reserved, 0.07G allocated, 15.46G free\n",
      "      Params      GFLOPs  GPU_mem (GB)  forward (ms) backward (ms)                   input                  output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     3011043       4.015         0.294         31.34          48.6        (1, 3, 448, 448)                    list\n",
      "     3011043        8.03         0.382         17.72         32.54        (2, 3, 448, 448)                    list\n",
      "     3011043       16.06         0.518         54.21         86.45        (4, 3, 448, 448)                    list\n",
      "     3011043       32.12         0.797         15.86         11.12        (8, 3, 448, 448)                    list\n",
      "     3011043       64.24         1.315         24.72         14.14       (16, 3, 448, 448)                    list\n",
      "\u001b[34m\u001b[1mAutoBatch: \u001b[0mUsing batch-size 129 for CUDA:0 9.49G/15.99G (59%) \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 29.010.3 MB/s, size: 94.1 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning D:\\homework\\DS223\\final-project-ZhangZwaa-1\\yolov8_pascal_person_data\\labels\\train.cache... 2142 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2142/2142 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 43.917.9 MB/s, size: 134.7 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\homework\\DS223\\final-project-ZhangZwaa-1\\yolov8_pascal_person_data\\labels\\val.cache... 2232 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2232/2232 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to d:\\homework\\runs\\detect\\yolov8n_person_voc\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0010078125), 63 bias(decay=0.0)\n",
      "Image sizes 448 train, 448 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1md:\\homework\\runs\\detect\\yolov8n_person_voc\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/30      7.64G      1.101      2.214      1.199        359        448: 100%|██████████| 17/17 [00:04<00:00,  3.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.867      0.462      0.595      0.385\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/30       7.6G        1.1       1.29      1.206        386        448: 100%|██████████| 17/17 [00:03<00:00,  4.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.763       0.48      0.577      0.375\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/30      7.55G      1.143      1.255      1.226        357        448: 100%|██████████| 17/17 [00:03<00:00,  4.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:07<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.602      0.277      0.374      0.173\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/30      7.58G      1.195      1.286      1.269        370        448: 100%|██████████| 17/17 [00:03<00:00,  4.42it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.492      0.366      0.358      0.183\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/30      7.63G      1.233      1.284      1.284        363        448: 100%|██████████| 17/17 [00:03<00:00,  4.76it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.619       0.23      0.315      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/30      7.47G      1.276      1.309      1.312        342        448: 100%|██████████| 17/17 [00:03<00:00,  4.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.263      0.251      0.165      0.058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/30      7.55G      1.268       1.31      1.313        394        448: 100%|██████████| 17/17 [00:03<00:00,  4.78it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110       0.37      0.365        0.3      0.132\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/30      7.69G      1.249       1.23      1.304        342        448: 100%|██████████| 17/17 [00:03<00:00,  4.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.528      0.368      0.384      0.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/30       7.6G      1.224      1.205      1.299        403        448: 100%|██████████| 17/17 [00:03<00:00,  4.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110       0.56      0.459      0.472      0.238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/30      7.57G      1.205      1.183      1.283        359        448: 100%|██████████| 17/17 [00:03<00:00,  5.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.678      0.531      0.581      0.316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/30      7.57G      1.197      1.146      1.272        349        448: 100%|██████████| 17/17 [00:03<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.693      0.569      0.642      0.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/30      7.59G      1.174      1.117      1.258        299        448: 100%|██████████| 17/17 [00:03<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110       0.67        0.6      0.644      0.359\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/30      7.56G      1.152      1.081      1.246        385        448: 100%|██████████| 17/17 [00:03<00:00,  5.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.764      0.617      0.708      0.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/30      7.46G      1.145      1.056       1.24        366        448: 100%|██████████| 17/17 [00:03<00:00,  4.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.749      0.619      0.695      0.403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/30      7.57G      1.124      1.037      1.238        372        448: 100%|██████████| 17/17 [00:03<00:00,  4.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.759      0.588      0.675      0.409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/30      7.45G      1.092      1.025      1.214        314        448: 100%|██████████| 17/17 [00:03<00:00,  4.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.717      0.554      0.641      0.389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/30      7.57G      1.115      1.011      1.216        347        448: 100%|██████████| 17/17 [00:03<00:00,  5.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.772      0.609      0.699      0.424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/30      7.46G      1.071     0.9751        1.2        353        448: 100%|██████████| 17/17 [00:03<00:00,  4.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.755       0.57      0.656      0.396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/30      7.56G      1.049     0.9536      1.186        422        448: 100%|██████████| 17/17 [00:03<00:00,  5.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.798      0.656       0.74      0.467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/30      7.56G      1.025     0.9314      1.179        342        448: 100%|██████████| 17/17 [00:03<00:00,  5.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.832      0.665      0.773      0.501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/30      7.52G      1.014     0.9183      1.151        153        448: 100%|██████████| 17/17 [00:04<00:00,  4.23it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.783      0.635      0.731      0.471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/30      7.46G      1.003     0.8537       1.14        188        448: 100%|██████████| 17/17 [00:03<00:00,  5.08it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.774      0.652      0.737      0.472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/30      7.53G     0.9822     0.8033       1.12        212        448: 100%|██████████| 17/17 [00:03<00:00,  5.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.827      0.669      0.775       0.51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/30      7.53G     0.9565     0.7607      1.107        156        448: 100%|██████████| 17/17 [00:03<00:00,  4.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.812       0.68      0.775      0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/30      7.52G     0.9325     0.7368      1.088        173        448: 100%|██████████| 17/17 [00:03<00:00,  5.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.823      0.708      0.798      0.519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/30      7.45G      0.914       0.72      1.078        189        448: 100%|██████████| 17/17 [00:03<00:00,  5.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.836      0.692      0.789      0.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/30      7.44G     0.8963     0.7011      1.073        157        448: 100%|██████████| 17/17 [00:03<00:00,  5.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110       0.85      0.692        0.8       0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/30      7.52G     0.8717     0.6648      1.065        187        448: 100%|██████████| 17/17 [00:03<00:00,  5.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.838      0.705      0.804      0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/30      7.53G     0.8553     0.6574      1.056        159        448: 100%|██████████| 17/17 [00:03<00:00,  5.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.838      0.717      0.809      0.546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/30      7.55G     0.8394     0.6394      1.035        165        448: 100%|██████████| 17/17 [00:03<00:00,  5.12it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:04<00:00,  1.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.838      0.722      0.813      0.551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "30 epochs completed in 0.077 hours.\n",
      "Optimizer stripped from d:\\homework\\runs\\detect\\yolov8n_person_voc\\weights\\last.pt, 6.2MB\n",
      "Optimizer stripped from d:\\homework\\runs\\detect\\yolov8n_person_voc\\weights\\best.pt, 6.2MB\n",
      "\n",
      "Validating d:\\homework\\runs\\detect\\yolov8n_person_voc\\weights\\best.pt...\n",
      "Ultralytics 8.3.151  Python-3.12.10 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4080 SUPER, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 9/9 [00:05<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.842      0.719      0.813      0.551\n",
      "Speed: 0.0ms preprocess, 0.2ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1md:\\homework\\runs\\detect\\yolov8n_person_voc\u001b[0m\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "print(\"Loading YOLOv8n model...\")\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "print(\"Starting YOLOv8 model training...\")\n",
    "results = model.train(\n",
    "    data='D:\\\\homework\\\\DS223\\\\final-project-ZhangZwaa-1\\\\yolov8_pascal_person_data\\\\a.yaml',\n",
    "    epochs=30,                 # Number of training epochs (adjust as needed)\n",
    "    imgsz=IMAGE_SIZE[0],       # Image size for training (e.g., 448 for 448x448)\n",
    "    batch=-1,                  # AutoBatch (automatically determines batch size)\n",
    "    name='yolov8n_person_voc'  # Name for your training run results folder\n",
    ")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc52678",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7003eb85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded trained model from: D:\\homework\\runs\\detect\\yolov8n_person_voc\\weights\\best.pt\n",
      "\n",
      "Validating the trained model...\n",
      "Ultralytics 8.3.151  Python-3.12.10 torch-2.7.0+cu126 CUDA:0 (NVIDIA GeForce RTX 4080 SUPER, 16376MiB)\n",
      "Model summary (fused): 72 layers, 3,005,843 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1474.8611.9 MB/s, size: 112.2 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning D:\\homework\\DS223\\final-project-ZhangZwaa-1\\yolov8_pascal_person_data\\labels\\val.cache... 2232 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2232/2232 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 140/140 [00:05<00:00, 24.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       2232       5110      0.846      0.719      0.814      0.551\n",
      "Speed: 0.0ms preprocess, 0.3ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1md:\\homework\\runs\\detect\\val11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load your best trained model (usually saved as 'runs/detect/your_run_name/weights/best.pt')\n",
    "# Replace 'path/to/your/best.pt' with the actual path\n",
    "# The 'results' object from model.train() might also contain the path\n",
    "try:\n",
    "    # If training was successful, results is a list of Results objects.\n",
    "    # Use the first result's save_dir to get the run directory.\n",
    "    # trained_model_path = os.path.join(results[0].save_dir, 'weights', 'best.pt')\n",
    "    trained_model_path = \"D:\\\\homework\\\\runs\\\\detect\\\\yolov8n_person_voc\\\\weights\\\\best.pt\" \n",
    "    model = YOLO(trained_model_path)\n",
    "    print(f\"Loaded trained model from: {trained_model_path}\")\n",
    "except:\n",
    "    # If you're running this part separately or restarted your kernel,\n",
    "    # manually specify the path to your trained model.\n",
    "    print(\"Loading trained model from a default path (if training was run previously)...\")\n",
    "    model = YOLO('runs/detect/yolov8n_person_voc/weights/best.pt') # Adjust if your run name differs\n",
    "\n",
    "# Validate the model on the validation set\n",
    "print(\"\\nValidating the trained model...\")\n",
    "metrics = model.val(data='D:\\\\homework\\\\DS223\\\\final-project-ZhangZwaa-1\\\\yolov8_pascal_person_data\\\\a.yaml') # Use the same dataset config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2618228f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mAP50: 0.8042\n",
      "mAP50-95: 0.5421\n",
      "Precision: 0.8363\n",
      "Recall: 0.7119\n",
      "\n",
      "Making a prediction on a sample image...\n",
      "Sample image not found at ./yolov8_pascal_person_data\\images\\val\\2008_000028.jpg. Please provide a valid path to an image for prediction.\n"
     ]
    }
   ],
   "source": [
    "# Print main detection metrics (mAP50, mAP50-95, precision, recall)\n",
    "print(f\"mAP50: {metrics.box.map50:.4f}\")\n",
    "print(f\"mAP50-95: {metrics.box.map:.4f}\")\n",
    "print(f\"Precision: {metrics.box.mp:.4f}\")\n",
    "print(f\"Recall: {metrics.box.mr:.4f}\")\n",
    "\n",
    "# Make predictions on an image (e.g., a sample from your validation set)\n",
    "# Replace 'path/to/your/image.jpg' with an actual image path you want to test\n",
    "print(\"\\nMaking a prediction on a sample image...\")\n",
    "# You might need to copy an image from your val set, e.g.,\n",
    "# from yolov8_pascal_person_data/images/val/\n",
    "sample_image_path = os.path.join(OUTPUT_YOLO_DIR, 'images', 'val', '2008_000028.jpg') # Example from VOC\n",
    "if os.path.exists(sample_image_path):\n",
    "    predict_results = model.predict(source=sample_image_path, save=True, conf=0.7) # conf is confidence threshold\n",
    "    print(f\"Prediction results saved to: {predict_results[0].save_dir}\")\n",
    "    print(f\"Detected objects: {predict_results[0].boxes}\")\n",
    "else:\n",
    "    print(f\"Sample image not found at {sample_image_path}. Please provide a valid path to an image for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8abc3ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded model from: D:\\homework\\runs\\detect\\yolov8n_person_voc\\weights\\best.pt\n",
      "\n",
      "Starting object recognition on images in: D:/homework/DS223/final-project-ZhangZwaa-1/testdata\n",
      "Detected images with bounding boxes will be saved to a subfolder within 'runs/detect/'.\n",
      "\n",
      "image 1/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\DSC00009.JPG: 320x448 1 person, 3.5ms\n",
      "image 2/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\DSC01527.png: 320x448 (no detections), 3.1ms\n",
      "image 3/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\DSC01704.png: 448x320 (no detections), 3.4ms\n",
      "image 4/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\DSC01751.png: 448x320 2 persons, 3.1ms\n",
      "image 5/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\DSC01767.png: 448x320 1 person, 4.8ms\n",
      "image 6/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\IMG_4132.JPG: 448x320 1 person, 4.4ms\n",
      "image 7/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\IMG_5420.JPG: 352x448 5 persons, 5.8ms\n",
      "image 8/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\Simpson.png: 256x448 (no detections), 4.8ms\n",
      "image 9/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\doge.jpg: 320x448 (no detections), 3.7ms\n",
      "image 10/10 D:\\homework\\DS223\\final-project-ZhangZwaa-1\\testdata\\k1.JPG: 448x352 1 person, 3.8ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 352)\n",
      "Results saved to \u001b[1md:\\homework\\runs\\detect\\inference_results_folder\u001b[0m\n",
      "\n",
      "Recognition complete. Processing detailed results:\n",
      "\n",
      "--- Image 1: DSC00009.JPG ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.82\n",
      "    Bounding Box (xyxy): [1470, 982, 4736, 4492]\n",
      "\n",
      "--- Image 2: DSC01527.png ---\n",
      "  No objects detected.\n",
      "\n",
      "--- Image 3: DSC01704.png ---\n",
      "  No objects detected.\n",
      "\n",
      "--- Image 4: DSC01751.png ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.93\n",
      "    Bounding Box (xyxy): [2083, 1536, 3758, 7008]\n",
      "  Object 2:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.87\n",
      "    Bounding Box (xyxy): [347, 1676, 2449, 6913]\n",
      "\n",
      "--- Image 5: DSC01767.png ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.91\n",
      "    Bounding Box (xyxy): [533, 1876, 3494, 6036]\n",
      "\n",
      "--- Image 6: IMG_4132.JPG ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.90\n",
      "    Bounding Box (xyxy): [1082, 2246, 1707, 4107]\n",
      "\n",
      "--- Image 7: IMG_5420.JPG ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.93\n",
      "    Bounding Box (xyxy): [568, 163, 1311, 2580]\n",
      "  Object 2:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.90\n",
      "    Bounding Box (xyxy): [0, 631, 568, 3024]\n",
      "  Object 3:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.89\n",
      "    Bounding Box (xyxy): [2854, 47, 3510, 2708]\n",
      "  Object 4:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.88\n",
      "    Bounding Box (xyxy): [1266, 283, 2004, 2583]\n",
      "  Object 5:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.87\n",
      "    Bounding Box (xyxy): [3378, 119, 4032, 2889]\n",
      "\n",
      "--- Image 8: Simpson.png ---\n",
      "  No objects detected.\n",
      "\n",
      "--- Image 9: doge.jpg ---\n",
      "  No objects detected.\n",
      "\n",
      "--- Image 10: k1.JPG ---\n",
      "  Object 1:\n",
      "    Class: person (ID: 0)\n",
      "    Confidence: 0.94\n",
      "    Bounding Box (xyxy): [558, 578, 792, 1083]\n",
      "\n",
      "Result images with detections saved to: d:\\homework\\runs\\detect\\inference_results_folder\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to your trained YOLOv8 model weights.\n",
    "# Ensure this path is absolutely correct.\n",
    "TRAINED_MODEL_PATH = 'D:\\\\homework\\\\runs\\\\detect\\\\yolov8n_person_voc\\\\weights\\\\best.pt' \n",
    "\n",
    "# Path to the folder containing the images you want to recognize.\n",
    "# The model will process all image files (jpg, png, etc.) within this directory.\n",
    "IMAGE_FOLDER_PATH = 'D:/homework/DS223/final-project-ZhangZwaa-1/testdata' \n",
    "\n",
    "# --- Load the trained YOLOv8 model ---\n",
    "try:\n",
    "    model = YOLO(TRAINED_MODEL_PATH)\n",
    "    print(f\"Successfully loaded model from: {TRAINED_MODEL_PATH}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model from {TRAINED_MODEL_PATH}: {e}\")\n",
    "    print(\"Please ensure TRAINED_MODEL_PATH is correct and the model file exists.\")\n",
    "    exit() # Exit the script if the model cannot be loaded\n",
    "\n",
    "# --- Perform inference on all images in the specified folder ---\n",
    "# This single call to model.predict() will efficiently process all images\n",
    "# within the IMAGE_FOLDER_PATH.\n",
    "print(f\"\\nStarting object recognition on images in: {IMAGE_FOLDER_PATH}\")\n",
    "print(\"Detected images with bounding boxes will be saved to a subfolder within 'runs/detect/'.\")\n",
    "\n",
    "results = model.predict(\n",
    "    source=IMAGE_FOLDER_PATH,  # The input source is the folder containing images\n",
    "    save=True,                 # Save the results (images with detections) to disk\n",
    "    conf=0.7,                  # Confidence threshold: only show detections with confidence >= 0.7\n",
    "    iou=0.3,                   # IoU threshold for Non-Maximum Suppression (NMS)\n",
    "                               # Lower this value to reduce overlapping duplicate boxes.\n",
    "    show=False,                # Set to True to display results in pop-up windows (requires OpenCV).\n",
    "                               # Set to False to just save them.\n",
    "    name='inference_results_folder' # Name for the results subfolder in 'runs/detect/'\n",
    ")\n",
    "\n",
    "# --- Process and print textual results for each image ---\n",
    "print(\"\\nRecognition complete. Processing detailed results:\")\n",
    "for i, result in enumerate(results):\n",
    "    # 'result' object contains predictions for one image\n",
    "    original_image_path = result.path # Path to the original image\n",
    "    detected_boxes = result.boxes     # Access the Boxes object for bounding box outputs\n",
    "\n",
    "    print(f\"\\n--- Image {i+1}: {os.path.basename(original_image_path)} ---\")\n",
    "    \n",
    "    if len(detected_boxes) == 0:\n",
    "        print(\"  No objects detected.\")\n",
    "    else:\n",
    "        for j, box in enumerate(detected_boxes):\n",
    "            class_id = int(box.cls[0])       # Class ID (e.g., 0 for 'person')\n",
    "            confidence = float(box.conf[0])  # Confidence score (e.g., 0.95)\n",
    "            # Bounding box coordinates in [x1, y1, x2, y2] format\n",
    "            # x1, y1 are top-left; x2, y2 are bottom-right\n",
    "            xyxy_coords = box.xyxy[0].tolist() \n",
    "            \n",
    "            # Get the class name from the model's loaded names\n",
    "            class_name = model.names[class_id] \n",
    "            \n",
    "            print(f\"  Object {j+1}:\")\n",
    "            print(f\"    Class: {class_name} (ID: {class_id})\")\n",
    "            print(f\"    Confidence: {confidence:.2f}\")\n",
    "            print(f\"    Bounding Box (xyxy): [{xyxy_coords[0]:.0f}, {xyxy_coords[1]:.0f}, {xyxy_coords[2]:.0f}, {xyxy_coords[3]:.0f}]\")\n",
    "\n",
    "# --- Information about where results are saved ---\n",
    "if results:\n",
    "    # results[0].save_dir contains the path to the folder where processed images are saved\n",
    "    print(f\"\\nResult images with detections saved to: {results[0].save_dir}\")\n",
    "else:\n",
    "    print(\"\\nNo results were generated. Check if the IMAGE_FOLDER_PATH contains valid images.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
